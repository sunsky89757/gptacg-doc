# 最佳实践

本指南提供了一套全面的最佳实践，帮助您从原型过渡到生产环境。无论您是资深的机器学习工程师，还是新近的爱好者，本指南都将为您提供在生产环境中成功运行平台所需的工具：从确保访问我们的API到设计能够处理高流量的强大架构。请使用本指南制定计划，尽可能顺利有效地部署您的应用程序。

## 影响延迟的因素及缓解建议

以下是影响延迟的主要因素，以及可能的缓解策略：

### 模型选择

我们提供了复杂性和通用性各异的模型。能力最强的模型，如`gpt-4`，生成过程较慢但结果多样。更简单模型如`gpt-4o-mini`则响应更快，但可能在准确性或相关性方面略逊一筹。根据您的用例权衡选择合适的模型。

### 完成tokens的数量

生成大量tokens会增加延迟。建议：

- 降低max_tokens参数，以减少延迟。
- 使用停止序列来防止生成不必要的tokens。
- 减少`n`和`best_of`的值，即每个提示生成的完成数量和用于对比的完成数量。

### 流式传输

设置`stream: true`可以让模型在tokens生成后立即返回，这减少了第一个token到达的时间。这不改变获取所有tokens的时间，但对于希望展示部分进度的应用，这是一个改善用户体验的好方法。

### 基础设施

我们的服务器位于香港。为了减少与聚合AI服务器的通信延迟，建议将基础设施的相关部分放在大陆或者香港及周边区域。

### 批处理

根据您的用例，批处理请求可能有助于降低请求数量。提示参数可容纳最多20个唯一提示。测试这种方法是否有效，不过在某些情况下，生成的tokens可能会增加，导致响应时间变长。

## 成本管理

在将原型投入生产时，应为运行应用程序的成本做预算。每1000个tokens约合750字计费。要估算成本，需要预测token使用量，考虑流量水平、用户与应用程序的交互频率以及处理的数据量。

降低成本的策略可通过减少tokens的使用或选择较小的模型来实现。可以通过缩短提示、微调模型或缓存通用用户查询，以减少token使用。

使用[OpenAI的交互式分词工具>>](https://platform.openai.com/tokenizer)来估算成本。在已选择最强模型进行测试后，您可以尝试其他模型，看其是否能以较低的延迟和成本生成相同的结果。

## 安全和合规

投入生产时，需评估并处理任何适用的安全和合规要求。这包括处理数据时的安全措施，了解API的数据处理方式，以及需要遵循的法规。[安全和合规文档>>](https://openai.com/policies/privacy-policy/)。

常见考虑包括数据存储、传输及保留。可能需实施数据隐私保护（如加密或匿名化）并遵循安全编码的最佳实践。

## 安全最佳实践

开发应用程序时，遵循安全最佳实践以保证应用程序的安全与成功。建议广泛测试产品、积极解决潜在问题并限制误用的机会。
